#!/home/detchar/opt/gwpysoft/bin/python
#
# Copyright (C) 2009 Tomoki Isogai
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
%prog -i ini_file [options]

Tomoki Isogai (isogait@carleton.edu)

This program parses param file, sets up a segment file, and creates dag file.
"""
# =============================================================================
#
#                               PREAMBLE
#
# =============================================================================

import os
import sys
import optparse
import ConfigParser
import time
import math
import glob
import re
import urllib2
import shutil
from numpy import sqrt

from glue import pipeline
from glue import segmentsUtils
from glue.segments import segment, segmentlist
from glue.iterutils import *
sys.path.append("/home/sudarshan.ghonge/bcv_pipeline/bcv")
from bcv import git_version
from bcv import bcvUtils

__author__ = "Tomoki Isogai <isogait@carleton.edu>"
__date__ = "12/25/2009"
__version__ = "1.0"

def parse_commandline():
    """
    Parse the options given on the command-line.
    """
    parser = optparse.OptionParser(usage=__doc__,version=git_version.verbose_msg)

    parser.add_option("-i", "--ini_file", help="file which contains parameters")
    parser.add_option("-s", "--write_script",action="store_true", default=False,
                      help="create a script in addition to dag file")
    parser.add_option("-v", "--verbose", action="store_true",\
                      default=False, help="run verbosely")
    parser.add_option("-k", "--KW_location", help="Location of KW triggers")
    
    parser.add_option("-g", "--GW_channel", help="GW channel (for use with omicron)")
    #parser.add_option("-k", "--Fast_channel", help="Fast channel (for use with omicron)")
    # parser.add_option("-t", "--Trigger_type", help="Type of trigger to retreive")    
    # parse the options
    opts, args = parser.parse_args()
    
    # check if necessary input exists
    if opts.ini_file is None:
        print >>sys.stderr, "--ini-file is a required parameter"
        sys.exit(1)
    if not os.path.isfile(opts.ini_file):
        print >>sys.stderr, "ini file not found"
        sys.exit(1)    
        
    return opts

################################################################################
# Define Jobs.  A Job corresponds to a program.  Each Job can have multiple 
# Nodes (instances).
################################################################################

class omegavetoJob(pipeline.CondorDAGJob):
  """
  This class represents the omegaveto.m. 
  """
  def __init__(self, cp):
    """
    cp = a ConfigParser instance
    """
    self.__executable = cp.get('condor','omegaveto_wrapper')
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe, self.__executable)

    self.set_stdout_file('logs/$(logname)-$(cluster).out')
    self.set_stderr_file('logs/$(logname)-$(cluster).err')
    self.add_condor_cmd('getenv','True')
    #self.add_condor_cmd('requirements','Memory >= %s'%cp.get("condor","omegavetoRequiredMemory"))

    self.add_opt("omegaveto_bin",cp.get("condor","compiled_omegaveto"))
    self.add_opt("highPassCutoff",cp.get("data_conditioning","highPassCutoff"))
    self.add_opt("logFile",cp.get("output","logFile"))
    if cp.getboolean("output","generateReport"):
      self.add_opt("generateReport","")
    self.add_opt("timeShiftMin",cp.get("data_conditioning","timeShiftMin"))
    self.add_opt("timeShiftMax",cp.get("data_conditioning","timeShiftMax"))
    self.add_opt("numTimeShifts",cp.get("data_conditioning","numTimeShifts"))
    self.add_opt("debugLevel",cp.get("output","debugLevel"))
    self.add_opt("sampleFrequency", cp.get("data_conditioning", "sampleFrequency"))

    
class reportJob(pipeline.CondorDAGJob):
  """
  This class represents the veto_report program.
  veto_report creats a webpage that summarizes the result for a 
  channel.
  """
  def __init__(self, cp):
    """
    cp = a ConfigParser instance
    """
    self.__executable = cp.get('condor','veto_report')
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe, self.__executable)

    self.set_stdout_file('logs/$(logname)-$(cluster).out')
    self.set_stderr_file('logs/$(logname)-$(cluster).err')
    self.add_condor_cmd('getenv','True')
    #self.add_condor_cmd('requirements','Memory >= %s'%cp.get("condor","reportRequiredMemory"))

    self.add_opt("ini_file", opts.ini_file)
    self.add_opt("HTriggerFile",GWtrigFile)
    self.add_opt("segment_file", segment_file)
    self.add_opt("out_dir",cp.get("webpage","outdir"))
    self.add_opt("scratch_dir",cp.get("condor","scratchdir"))
    self.add_opt("positive_window",cp.get("data_conditioning","positive_window"))
    self.add_opt("negative_window",cp.get("data_conditioning","negative_window"))
    self.add_opt("reqAccVetoRate",cp.get("data_conditioning","reqAccVetoRate"))
    self.add_opt("vetoSignThresh",cp.get("data_conditioning","vetoSignificanceThresh"))
    if cp.get("input","injection_file") != "off":
      self.add_opt("injection_file",cp.get("input","injection_file"))
      self.add_opt("safety_thresh",cp.get("data_conditioning","safety_thresh"))
    if cp.getboolean("data_conditioning","rMax"):
      self.add_arg("--use_rMax")
    self.add_arg("--verbose")
    
class summaryPageJob(pipeline.CondorDAGJob):
  """
  This class represents the veto_summaryPage program. 
  veto_summaryPage creates a summary page for all channels analyzed: 
  it lists veto candidate channels and shows important info like 
  use percentage, veto efficiency etc. and links to individual 
  channel summary page.
  """
  def __init__(self, cp):
    """
    cp = a ConfigParser instance
    """
    self.__executable = cp.get('condor','veto_summaryPage')
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe, self.__executable)

    self.set_stdout_file('logs/$(logname)-$(cluster).out')
    self.set_stderr_file('logs/$(logname)-$(cluster).err')
    self.add_condor_cmd('getenv','True')
  
    self.add_opt("result_dir", cp.get("output","outDir"))
    self.add_opt("ini_file", opts.ini_file)
    self.add_opt("HTriggerFile",GWtrigFile)
    self.add_opt("out_dir", cp.get("webpage", "outdir"))
    self.add_opt("scratch_dir",cp.get("condor","scratchdir"))
    self.add_arg("--verbose")

class insertJob(pipeline.CondorDAGJob):
  """
  """
  def __init__(self, cp):
    """
    cp = a ConfigParser instance
    """
    self.__executable = cp.get('condor','veto_insert-bin')
    self.__universe = "local"
    pipeline.CondorDAGJob.__init__(self,self.__universe, self.__executable)

    self.set_stdout_file('logs/$(logname)-$(cluster).out')
    self.set_stderr_file('logs/$(logname)-$(cluster).err')
    self.add_condor_cmd('getenv','True')
  
    self.add_opt("result_dir", cp.get("output","outDir"))
    if cp.getboolean("veto","xml"):
      self.add_opt("out_dir",os.path.join(web_outdir,"inserted_segs"))
      self.add_arg("--write_xml")
    if cp.getboolean("veto","insert"):
      self.add_opt("out_dir",os.path.join(web_outdir,"inserted_segs"))
      self.add_arg("--insert")
    self.add_opt("segment-url", cp.get("veto","server"))
    self.add_opt("web_address",cp.get("veto","web_address"))
    for u in unsafe_list:
      self.add_arg("--unsafe_channels %s"%u)
    self.add_opt("scratch_dir",cp.get("condor","scratchdir"))
    self.add_opt("trigger_type",cp.get("veto","trigger_type"))
    self.add_arg("--verbose")
      
'''
     
class wscanRunJob(pipeline.CondorDAGJob):
  def __init__(self, cp):
      self.__executable = cp.get('condor','KW_veto_runOmega-bin')
      self.__universe = "local"
      pipeline.CondorDAGJob.__init__(self,self.__universe, self.__executable)
      self.set_stdout_file('logs/$(logname)-$(cluster).out')
      self.set_stderr_file('logs/$(logname)-$(cluster).err')
      self.add_condor_cmd('getenv','True')
      
      output = os.path.join(web_outdir,"followup")
      self.add_opt("time_file",os.path.join(output,"omega_times.txt"))
      self.add_opt("w_executable",cp.get("condor","wscan-bin"))
      self.add_opt("w_config",cp.get("follow_up","wscan_config"))
      self.add_opt("w_framecache",cp.get("follow_up","wscan_framecache"))
      self.add_opt("out_dir",os.path.join(output,"omega"))
      self.add_arg("--verbose")
'''   
        
################################################################################
# Define Nodes.  A Node corresponds to a single instance of a program to be
# run.  They each attach to a Job, which contains the information common to
# all Nodes of a single type.
################################################################################

class omegavetoNode(pipeline.CondorDAGNode):
  def __init__(self, job, name, retry, segfile_name, configFile, frameCacheFileH, frameCacheFileX, frameCacheFileY, outDir, filePrefix, SNRcutoffH, SNRcutoffX):
    """
    A omegavetoNode runs an instance of omegaveto in a Condor DAG.
    """
    
    pipeline.CondorDAGNode.__init__(self, job)

    self.set_name(name)
    self.set_retry(retry)
    self.set_category("omegaveto")
    self.add_var_opt("segmentFile",segfile_name)
    self.add_var_opt("configurationFile",configFile)
    self.add_var_opt("frameCacheFileH",frameCacheFileH)
    self.add_var_opt("frameCacheFileX",frameCacheFileX)
    self.add_var_opt("SNRcutoffH",SNRcutoffH)
    self.add_var_opt("SNRcutoffX",SNRcutoffX)
    if frameCacheFileY is None: # linear coupling model
      self.add_var_opt("couplingModel",'linear')
      self.add_var_opt("frameCacheFileY",'None')
    else: # bilinear
      self.add_var_opt("couplingModel",'bilinear')
      self.add_var_opt("frameCacheFileY",frameCacheFileY)
    self.add_var_opt("outDir",outDir)
    self.add_macro("logname","omegaveto-"+filePrefix) # used for log name
        

class reportNode(pipeline.CondorDAGNode):
  def __init__(self, job, name, retry, outDir, filePrefix,couplingModel,SNRcutoffH,SNRcutoffX):
    """
    A reportNode runs an instance of veto_report in a Condor DAG.
    """
    
    pipeline.CondorDAGNode.__init__(self, job)
    

    self.set_name(name)
    self.set_retry(retry)
    self.set_category("report")
    self.add_macro("logname","report-"+filePrefix) # used for log name
    self.add_var_opt("couplingModel",couplingModel)
    self.add_var_opt("result_dir",outDir)
    self.add_var_opt("SNRcutoffH",SNRcutoffH)
    self.add_var_opt("SNRcutoffX",SNRcutoffX)
        
class summaryPageNode(pipeline.CondorDAGNode):
  def __init__(self,job,name,retry,filePrefix):
    """
    A summaryPageNode runs an instance of veto_summaryPage in 
    a Condor DAG.
    """
    
    pipeline.CondorDAGNode.__init__(self, job)

    self.set_name(name)
    self.set_retry(retry)
    self.add_macro("logname","summaryPage-"+filePrefix) # used for log name

class insertNode(pipeline.CondorDAGNode):
  def __init__(self, job, name, retry, filePrefix):
    """
    """
    
    pipeline.CondorDAGNode.__init__(self, job)

    self.set_name(name)
    self.set_retry(retry)
    self.add_macro("logname","insert-"+filePrefix) # used for log name
        
'''
        
        
class wscanRunNode(pipeline.CondorDAGNode):
    def __init__(self, job, name, retry, filePrefix, index):
        """
        """
        
        pipeline.CondorDAGNode.__init__(self, job)
        self.set_name(name)
        self.set_retry(retry)
        self.set_category("wscan")
        self.add_var_opt("index", index)
        self.add_macro("logname","wscanRun-"+filePrefix) # used for log name
        

'''     
        
################################################################################
# Manipulate segment files and get the actual segments to run the program on
################################################################################

def get_analyze_segment(ifo):
  """
  cp is a config parser instance

  This function does the following to get the segment list to be analyzed:
  - take a union of the segment files specified on analyzed_seg_files 
    in param file
  - subtract category specified 
  """
  if not cp.getboolean("condor","only_postprocess"):
    if opts.verbose: print "making segment list..."
  
    if not os.path.exists("segfiles"): os.makedirs("segfiles")
    
    ################# take a union of analyzed_seg_files ###################
    
    if opts.verbose: print "getting analyzed segments..."
    analyzed_seg_files = bcvUtils.get_files_from_globs(cp.get("input","analyzed_seg_files"))
    # check if there is at least one file
    if analyzed_seg_files == []:
        print >>sys.stderr, "Error: analyzed_seg_files not found."
        sys.exit(1)
        
    if opts.verbose: print "added seg files:", analyzed_seg_files
    analyzed_segs=segmentlist()
    # take a union of one segment list at a time
    for fileName in analyzed_seg_files:
      if fileName.endswith("txt"):
        segs = bcvUtils.read_segfile(fileName)
      elif fileName.endswith("xml") or fileName.endswith("xml.gz"):
        segs = bcvUtils.read_segfile_xml(fileName,opts.verbose)
      else:
        print >> sys.stderr, "Error: file format of %s is not supported."%fileName
        sys.exit(1)
      analyzed_segs.__ior__(segs)
    
    ####################### subtract flag_seg_files ########################
   
    if opts.verbose: print "subtracting flag_seg_files..." 
    flag_seg_files = bcvUtils.get_files_from_globs(cp.get("input","flag_seg_files")) 
    if opts.verbose: print "subtracted seg files:",flag_seg_files
    for fileName in flag_seg_files:
      if fileName.endswith("txt"):
        segs = bcvUtils.read_segfile(fileName)
      elif fileName.endswith("xml") or fileName.endswith("xml.gz"):
        segs = bcvUtils.read_segfile_xml(fileName)
      else:
        print >> sys.stderr, "Error: file format of %s is not supported."%fileName
        sys.exit(1)
      analyzed_segs = analyzed_segs - segs
  
    analyzed_segs.coalesce()
    print 'analysed_segs: ', analyzed_segs
  
    ######################### subtract category times ######################
  
    if any(cats.values()) == True:
      start = int(analyzed_segs[0][0])
      end = int(analyzed_segs[-1][1]+1)
      duration = end - start
      # FIXME: write my own function - ligolw_segments_from_cats_dqsegdb gets segments 
      # for all ifos and wasteful
      def get_cats():
        if opts.verbose:
          print "getting cat veto files..."
          print "this might take a while..."
        cmd = "ligolw_segments_from_cats_dqsegdb --veto-file %s --output-dir %s --segment-url %s --separate-categories --gps-start-time %d --gps-end-time %d"%(cp.get("input","veto_definer_file"),"segfiles",cp.get("input","server"),start,end)
        print cmd
        exit = os.system(cmd)
        if exit > 0:
          print >> sys.stderr, """
          Error: command below failed.
                 %s"""%cmd
          sys.exit(1)
      for i in cats.keys():
        if cats[i] == True:
          cat_file = "segfiles/%s-VETOTIME_CAT%d-%d-%d.txt"%(ifo,i,start,duration)  
          if not os.path.isfile(cat_file):
            get_cats()
            cat_xml = "segfiles/%s-VETOTIME_CAT%d-%d-%d.xml"%(ifo,i,start,duration)
            cmd = 'lwtprint %s -c start_time -c end_time -t segment -d " " > %s' %(cat_xml, cat_file)
            print cmd
            exit = os.system(cmd)
            if exit>0:
              print >> sys.stderr, """
              Error: command below failed.
               %s"""%cmd
              sys.exit(1)
          if opts.verbose: print "subtracting %s"%cat_file  
          analyzed_segs = analyzed_segs - bcvUtils.read_segfile(cat_file)
  
    analyzed_segs.coalesce()
    print analyzed_segs
  
    # divide up segments that are too long
    analysis_segments = segmentlist()
    
    # implementing an upper duration in terms of seconds
    #upper_seconds = 3600 * cp.getint("condor","duration_upper_limit")
    upper_seconds = cp.getint("condor","duration_upper_limit")      

    def divide_seg(seg,upper):
      """
      Divides seg to smaller chunck so that the duration of each segment does
      not exceed upper.
      Returns glue.segments.segmentlist 
      """
      seg_list = segmentlist()
      i = 0
      for i in range(int(math.floor(abs(seg)/upper))):
         seg_list.append(segment(seg[0]+i*upper,seg[0]+(i+1)*upper))
      seg_list.append(segment(seg[0]+i*upper,seg[-1]))
      return seg_list
  
    for s in analyzed_segs:
      analysis_segments += divide_seg(s,upper_seconds)
  
    if opts.verbose: 
        print "segs to be analyzed:"; print analyzed_segs
    # for name
    start_time = int(analyzed_segs[0][0])
    end_time = int(analyzed_segs[-1][1])
    duration = end_time - start_time      
  
    ########################### save the result ############################
    
    output_name = os.path.join("segfiles","%s-%s-%s-segs.txt"%(cp.get("general","tag"),str(start_time),str(duration)))
    if opts.verbose: print "saving the segment list in %s..."%output_name
    segmentsUtils.tosegwizard(open(output_name,"w"),analyzed_segs,\
                              coltype=float)

    ## if running postprocess only, retrieve the original segment
  else:
    segfile_candidates = [f for f in os.listdir("segfiles") if f.startswith(original_tag+"-") and f.endswith("-segs.txt")]
    if len(segfile_candidates) != 1:
      print >> sys.stderr, "Error: found more than one candidates for the original segfile."
      sys.exit(1)
    output_name = os.path.join("segfiles",segfile_candidates[0])
    analyzed_segs = bcvUtils.read_segfile(output_name)
    start_time = analyzed_segs[0][0]
    duration = abs(analyzed_segs)

  return start_time, duration, output_name, analyzed_segs

def config_reader():
  """
  Read a config file and return a dictionary:

  {"GW":(channelName,frameType,SNRcutoff)
   "fast":[(channelName,frameType,SNRcutoff), ...]
   "slow":[(channelName,frameType,SNRcutoff), ...]}

  """
  ## read in configuration file and sanity check
  config = ConfigParser.ConfigParser()
  config.read(cp.get("input","configuration_file"))

  # check the format of channel name; if it has acceptable ifo prefix
  for c in config.sections():
    if c[:2] not in ("H1","L1","V1"):
      print >> sys.stderr, "Error: all the channel names have to start from ${IFO}, i.e. H1, L1 or V1."
      sys.exit(1)

  # select the channels with the right ifo
  channels = [c for c in config.sections() if c[:2] == ifo]

  GW = []
  fast = []
  slow = []

  for c in channels:
    # check if necessary options exist and nothing else
    if config.options(c) != ['channeltype', 'frametype', 'snrcutoff']:
      print >> sys.stderr, "Error: channelType, frameType and SNRcutoff are required for each channel in config file."
      print >> sys.stderr, "       Check options for %s"%c
      sys.exit(1) 
    # if SNRcutoff is set to -1, set it to the value specified in .ini file
    if config.getfloat(c,"SNRcutoff") == -1:
      config.set(c,"SNRcutoff",cp.get("data_conditioning","SNRcutoff"))

    # read the data in a form (channelName, frameType, SNRcutoff)
    if config.get(c,"channelType") == "GW":
      GW.append((c,config.get(c,"frameType"),config.getfloat(c,"SNRcutoff")))
    elif config.get(c,"channelType") == "fast":
      fast.append((c,config.get(c,"frameType"),config.getfloat(c,"SNRcutoff")))
    elif config.get(c,"channelType") == "slow":
      slow.append((c,config.get(c,"frameType"),config.getfloat(c,"SNRcutoff")))
    else:
      print >> sys.stderr, "Error: unexpected channelType in config file."
      print >> sys.stderr, "       One of GW, fast or slow is expected."
      sys.exit(1)

  # we need exactly one GW channel
  if len(GW) != 1:
    print >> sys.stderr, "Error: exactly one GW channel is necessary."
    sys.exit(1)
  # we also need at least one channel (or LINEAR) in slow and fast channel
  if len(fast) < 1 or len(slow) < 1:
    print >> sys.stderr, "Error: at least one fast and slow channel is necessary."
    sys.exit(1)

  return {'GW':GW[0],'fast':fast,'slow':slow}

def config_maker():
  """
  Create confiburation file for omegaveto.
  In addition, get necessary KW triggers.
  """


  # make directory for configs
  if not cp.getboolean("condor","only_postprocess"):
    bcvUtils.rename("configs")
    os.mkdir("configs")

  
  # read the given configuration file
  if opts.verbose:
    print "reading %s..."%cp.get("input","configuration_file")
  chanData = config_reader()

  # get segments
  if opts.verbose: print "getting segments to be analyzed..."
  start_time, duration, segment_file, analyze_segments = get_analyze_segment(ifo)

  ## get KW for GW channel
  # make directory for KW files
  if not cp.getboolean("condor","only_postprocess"):
    bcvUtils.rename("KWtrigs")
    os.mkdir("KWtrigs")

  # if specified in ini file, use that location of KW
  if cp.get("input","KW_location") != "":
    KWLoc = "--KW_location %s"%cp.get("input","KW_location")
  else:
    KWLoc = "--KW_location %s"%opts.KW_location;

  # minimum SNR
  minSNRH = chanData['GW'][2]
  # verboseness
  if opts.verbose:
    verbose = "--verbose"
  else:
    verbose = ""

  #if cp.get("input","GW_trigger_file") == "KW":
    ## figure out GW channel name for KW
    #if cp.get("input","GW_KW") != "":
      #GWchan = cp.get("input","GW_KW")
    #elif ifo == "V1": # Virgo case
      #GWchan = "V1_PR_B1_ACP"
    #elif ifo in ("H1","H2","L1"): # LIGO case
      #GWchan = "%s:LSC-DARM_ERR"%ifo
    ## output file name
    #GWtrigFile = os.path.join("KWtrigs",original_tag+"_"+GWchan+".txt")
    ## where KW triggers are retreived (?)
    #if trigger_type == "OMICRON": 
    	##cmd = "%s %s --channel_name %s --segment_file %s --out_name %s --order_by 'centralTime asc' --scratch_dir %s --min_thresh %d %s"%(cp.get("condor","getKW"), KWLoc, GWchan, segment_file, GWtrigFile, cp.get("condor","scratchdir"), minSNRH, verbose)	
    	#if opts.verbose: # this won't be necessary 
      		#print "Omicron triggers selected for GW channel..."
      		##print cmd
    #elif trigger_type == "KW":
	#cmd = "%s %s --channel_name %s --segment_file %s --out_name %s --order_by 'centralTime asc' --scratch_dir %s --min_thresh %d --min_freq %f --max_freq %f %s"%(cp.get("condor","getKW"), KWLoc, GWchan, segment_file, GWtrigFile, cp.get("condor","scratchdir"), minSNRH, cp.get("data_conditioning","trigMinFreq"), cp.get("data_conditioning","trigMaxFreq"), verbose)
        #if opts.verbose:
                #print "retrieving KW triggers for GW channel..."
                #print cmd

    #if not cp.getboolean("condor","only_postprocess"):
      #exit = os.system(cmd)
    #else:
      #exit = 0
    #if exit > 0:
      #print >> sys.stderr, "Error: KW retrieval failed."
      #sys.exit(1)
  trigMinFreq = cp.getfloat("data_conditioning","trigMinFreq")
  trigMinFreq = trigMinFreq if trigMinFreq is not None else 0.0
  trigMaxFreq = cp.getfloat("data_conditioning","trigMaxFreq")
  trigMaxFreq = trigMaxFreq if trigMaxFreq is not None else 40000.0 

  print "trigger type is %s"%(trigger_type)
  if trigger_type == "KW":
    GWchan = cp.get("input","GW_KW")
    GWtrigFile = os.path.join("KWtrigs",original_tag+"_"+GWchan+".txt")
    cmd = "%s %s --channel_name %s --segment_file %s --out_name %s --order_by 'centralTime asc' --scratch_dir %s --min_thresh %d --min-freq %f --max-freq %f %s"%(cp.get("condor","getKW"), KWLoc, GWchan, segment_file, GWtrigFile, cp.get("condor","scratchdir"), minSNRH, cp.getfloat("data_conditioning","trigMinFreq"), cp.getfloat("data_conditioning","trigMaxFreq"), verbose)
    if opts.verbose:
      print "retrieving KW triggers for GW channel..."
      print cmd

    if not cp.getboolean("condor","only_postprocess"):
      exit = os.system(cmd)
    else:
      exit = 0
    if exit > 0:
      print >> sys.stderr, "Error: KW retrieval failed."
      sys.exit(1)      
  elif trigger_type == "OMICRON":
      trigger_file = cp.get("input","GW_OMICRON")
      tf_1 = trigger_file.split(":")
      OMpath = KWLoc.split(" ")
      OMpath = OMpath[1]
      print >> sys.stderr, "Omicron selected..."
      GWtrigFile = os.path.join("KWtrigs",cp.get("general","tag")+"_"+os.path.basename(trigger_file) + '.txt')
      print >> sys.stderr, "Expected name: %s"%GWtrigFile
      #cmd = "%s %s --channel_name %s --segment_file %s --out_name %s --order_by 'centralTime asc' --scratch_dir %s --min_thresh %d %s"%(cp.get("condor","getKW"), KWLoc, GWchan, segment_file, GWtrigFile, cp.get("condor","scratchdir"), minSNRH, verbose)
      cmd = "%s -c %s -s : -m %s -o Omicron -O %s -I %s -k %s -S %s -L %f -H %f"%(cp.get("condor","getKW"), tf_1[1], minSNRH, OMpath,tf_1[0], cp.get("general","tag"),cp.get("input","analyzed_seg_files"),cp.getfloat("data_conditioning","trigMinFreq"), cp.getfloat("data_conditioning","trigMaxFreq"))
      if opts.verbose:
                print "retrieving OMICRON triggers for GW channel..."
                print cmd
      if not cp.getboolean("condor","only_postprocess"):
      	exit = os.system(cmd)
      else:
      	exit = 0
      if exit > 0:
      	print >> sys.stderr, "Error: OMICRON retrieval failed."
      	sys.exit(1)

  else: # provided GW trigger file
    trigger_file = cp.get("input","GW_trigger_file")
    if not os.path.isfile(trigger_file):
      print >> sys.stderr, "Error: GW_trigger_file not found"
      sys.exit(1)
    ## read triggers and sanity check
    ## especially, later it's assumed that triggers are already trimmed by
    ## SNR cutoff and segment list

    GWtrigs = [line.split() for line in open(trigger_file) if (line.split() != [] and not line.startswith("%") and not line.startswith("#"))]

    ## check if there is triggers
    if len(GWtrigs) == 0:
      print >> sys.stderr, "Error: no triggers found. Please check your trigger file %s."%trigger_file
      sys.exit(1)

    ## check the number of columns
    if len(GWtrigs[0]) not in [5]:
        print >> sys.stderr, """
#Error:
  #The file format is expected to be the same as KW; 5 columns with
  #start GPS time: 1st column
  #end GPS time: 2nd column
  #central GPS time: 3rd column
  #central frequency: 4th column
  #SNR: 5th column
  #Please check your trigger file %s.
        #"""%trigger_file
        sys.exit(1)

    ### get times and snrs
    contents = []
    for trig in GWtrigs:
       	t = float(trig[2])
       	s = float(trig[4])
       	if t in analyze_segments:
            if s >= minSNRH and s == s: # check NaN
                contents.append("\t".join(trig))
    GWtrigFile = os.path.join("KWtrigs",cp.get("general","tag")+"_"+os.path.basename(trigger_file))
    open(GWtrigFile,'w').write("".join(contents))
    #else:  #  may not need this now -> omicron GW trigger files will be created manually
	##GWtrigFile = os.path.join("KWtrigs",cp.get("general","tag")+"_"+"H1_PSL-FSS_FAST_MON_OUT_DQ_omicron.txt")
	##GWtrigFile_1 = os.path.join("trigs_pre",cp.get("general","tag")+"_"+"H1_PSL-FSS_FAST_MON_OUT_DQ_omicron.txt")
	##shutil.copy2(GWtrigFile_1, GWtrigFile)
	#print >> sys.stderr, "Error: OMICRON trigger type selected in .ini file. Please set GW_trigger_file variable to OMICRON to use this trigger type."
 
  ## produce config file for omegaveto
  ## common header - useful info
  curTime = time.strftime('%m-%d-%Y %H:%M:%S',time.localtime())
  user = os.environ['USER']
  header = ["# id: %s: %s\n"%(curTime,user)]
  header.append("""
[Gravitational,Gravitational Wave Channel]

{
    channelName:                    '%s'
    frameType:                      '%s'
    sampleFrequency:                %d
    triggerListChH:                 '%s'
}
  """%(chanData["GW"][0],chanData["GW"][1],cp.getint("data_conditioning","sampleFrequency"),GWtrigFile))

  ## make configuration for each channel pair
  for chan1 in chanData["fast"]:
    content = []
    content += header
    ## get KW
    trigFile = os.path.join("KWtrigs",cp.get("general","tag")+"_"+chan1[0]+".txt")
    minSNRX = chan1[2]
     
    if trigger_type == 'KW':
    	cmd = "%s %s --channel_name %s --segment_file %s --out_name %s --order_by 'centralTime asc' --scratch_dir %s --min_thresh %d --min-freq %f --max-freq %f %s"%(cp.get("condor","getKW"), KWLoc, chan1[0], segment_file, trigFile, cp.get("condor","scratchdir"), minSNRX, cp.getfloat("data_conditioning","trigMinFreq"), cp.getfloat("data_conditioning","trigMaxFreq"), verbose)
    elif trigger_type == 'OMICRON':
	XchName = chan1[0].split(":")
	XchName = XchName[1]
    	#cmd = "%s -c %s -s : -m %s -o Omicron"%(cp.get("condor","getKW"), chan1[0], minSNRX)
	cmd = "%s -c %s -s : -m %s -o Omicron -O %s -I %s -k %s -S %s -L %f -H %f"%(cp.get("condor","getKW"), XchName, minSNRX, OMpath,tf_1[0], cp.get("general","tag"),cp.get("input","analyzed_seg_files"),cp.getfloat("data_conditioning","trigMinFreq"), cp.getfloat("data_conditioning","trigMaxFreq"))
    if opts.verbose:
      if trigger_type == 'KW':
      	print "retrieving KW triggers..."
      	print cmd
      else:
	print "Omicron triggers selected..."
	print cmd
    if not cp.getboolean("condor","only_postprocess"):
      exit = os.system(cmd)
    else:
      exit = 0
    if exit > 0:
      if trigger_type == 'KW':
      	print >> sys.stderr, "Error: KW retrieval failed."
      elif trigger_type == 'OMICRON':
	print >> sys.stderr, "Error: OMICRON retrieval failed."
      sys.exit(1)

    ## add configuration file content
    content.append("""
{ 
    channelName:                    '%s'
    frameType:                      '%s'
    sampleFrequency:                %d
    triggerListChX:                 '%s' 
    transferFunctionXtoH:           'null'
} 
  """%(chan1[0], chan1[1], cp.getint("data_conditioning","sampleFrequency"), trigFile))

    for chan2 in chanData["slow"]:
      config_name = os.path.join("configs","%s_%s+%s.conf"%(cp.get("general","tag"),chan1[0],chan2[0])) 
      if(chan2[0][3:] == "LINEAR"):
        print "Linear coupling model selected..."
        open(config_name,'w').write("".join(content))
      else:
        if cp.get("data_conditioning","pairing") == "subsystem":
          chan1_prefix = chan1[0].split("-")[0]
          chan2_prefix = chan2[0].split("-")[0]
          if chan1_prefix != chan2_prefix:
            print "Channels not of same subsystem"
            continue 
        ## add configuration file content
        bilinear_content = [l for l in content] # make a copy
        bilinear_content.append("""
{ 
    channelName:                    '%s'
    frameType:                      '%s'
    sampleFrequency:                %d
} 
        """%(chan2[0],chan2[1],cp.getint("data_conditioning","sampleFrequency")))
        open(config_name,'w').write("".join(bilinear_content))

        

  return start_time, duration, segment_file, analyze_segments, chanData, GWtrigFile

################################################################################
# Set up DAG 
################################################################################

def dag_maker():
    """
    This function creates a dag file and condor submission files
    
    ifo_list contains ifos on which vetoStats is to run (vetoStats is the 
    program that calculates all the necessary values of veto like 
    used percentage, veto efficiency, dead time percentage etc.)
    ifo_list=[] means no vetoStats (use results from a previous run)
    cp is a config parser instance
    """
    tag = cp.get("general", "tag")

    ## create directory for Condor output and error files
    bcvUtils.rename("logs") 
    os.mkdir("logs")

    ## create directory for cache files from ligo_data_find
    bcvUtils.rename("cache")
    os.mkdir("cache")

    ############################################################################
    # set dag
    ############################################################################
    
    dag=pipeline.CondorDAG(os.path.join(cp.get("condor","logdir"),"%s.log"%tag))
    dag.set_dag_file(os.path.join("dags",tag))  

    ############################################################################
    # set jobs and subfiles
    ############################################################################
    
    datafind_job = pipeline.LSCDataFindJob("cache","logs",cp)
    datafind_job.set_sub_file(os.path.join("dags","%s.datafind.sub" % tag))
    # --gaps option gives an error when there is a gap in returned frame cache
    # datafind_job.add_arg("--gaps")
    # hack - omegaveto can only read frame-cache and not lal-cache...
#    del datafind_job._CondorJob__options['lal-cache']   
#    datafind_job.add_opt("frame-cache","")
#   Update, the source has moved to python which has gwpy.
#   gwpy can read lal-cache
    omegaveto_job = omegavetoJob(cp)
    omegaveto_job.set_sub_file(os.path.join("dags","%s.omegaveto.sub"%tag))

    report_job = reportJob(cp)
    report_job.set_sub_file(os.path.join("dags","%s.report.sub"%tag))
     
    summaryPage_job = summaryPageJob(cp)
    summaryPage_job.set_sub_file(os.path.join("dags","%s.summaryPage.sub"%tag))

    insert_job = insertJob(cp)
    insert_job.set_sub_file(os.path.join("dags","%s.insert.sub"%tag))

    ''' 
    followup_job = followupJob(cp)
    followup_job.set_sub_file(os.path.join("dags","%s.followup.sub"%tag))

    wscanRun_job = wscanRunJob(cp)
    wscanRun_job.set_sub_file(os.path.join("dags","%s.wscanRun.sub"%tag))
    '''

    retry = cp.getint("condor","retry")

    # get all possible frame types and run ligo_data_find    
    frameTypes = set([chan[1] for chan in chanData['fast']+chanData['slow'] if chan[0][3:]!="LINEAR"])

    # store parents nodes
    summary_parents = []
    report_parents = {}

    # chunk together a few segments so that each job have enough computational
    # cost (to avoid jamming a scheduler machine.)
    duration = 0
    segment_chunks = []
    segment_chunk = segmentlist()
    for s in analyze_segments:
      segment_chunk.append(s)
      duration += abs(s)
      #if duration > 3600 * cp.getint("condor","duration_lower_limit"):
      # changing this to work in terms of seconds, not hours
      if duration > cp.getint("condor","duration_lower_limit"):
        segment_chunks.append(segment_chunk)
        segment_chunk = segmentlist() 
        duration = 0
    if len(segment_chunk) > 0:
      segment_chunks.append(segment_chunk)

    # when jobs are run in chronological order, the same frame file can be 
    # accessed by multiple jobs at the same time, and it could choke the
    # workflow
    # randamize the job sequence to go around this problem.      
    import random
    random.shuffle(segment_chunks)

    for i, seg_chunk in enumerate(segment_chunks):
      # seg_chunk is glue.segments.segmentlist
      # figure out start and end time to construct a unique name
      start = seg_chunk[0][0]
      end = seg_chunk[-1][-1]

      # create segment list file for this chunk
      segfile_name = "segfiles/%s_%d_%d_segs.txt"%(tag,start,end)
      segmentsUtils.tosegwizard(open(segfile_name,"w"),seg_chunk,header=False)

      dfH = pipeline.LSCDataFindNode(datafind_job)
      dfH.set_name("datafind-GW-%d-%d"%(start,end))
      dfH.set_retry(retry)
      dfH.set_observatory(ifo.upper()[0])
      dfH.set_start(start)
      dfH.set_end(end)
      dfH.set_type(chanData['GW'][1])
      if not cp.getboolean("condor","only_postprocess"):
        dag.add_node(dfH)
  
      dfXs = {}
      for ft in frameTypes:
        dfX = pipeline.LSCDataFindNode(datafind_job)
        dfX.set_name("datafind-%s-%d-%d"%(ft,start,end))
        dfX.set_retry(retry)
        dfX.set_observatory(ifo.upper()[0])
        dfX.set_start(start)
        dfX.set_end(end)
        dfX.set_type(ft)
        if not cp.getboolean("condor","only_postprocess"):
          dag.add_node(dfX)  
        dfXs[ft] = dfX

      # chan1/2 are tuple (channelName, frameType, SNRcutoff)
      for chan1 in chanData['fast']:
        # bilinear case
        for chan2 in chanData['slow']:
          if(chan2[0][3:]=="LINEAR"):
            chan2Frame = None
          else:
            if(cp.get("data_conditioning" ,"pairing")=="subsystem"):
              chan1_prefix = chan1[0].split("-")[0]
              chan2_prefix = chan2[0].split("-")[0]
              if chan1_prefix != chan2_prefix:
                print "Channels not of same subsystem"
                continue
            chan2Frame = dfXs[chan2[1]].get_output()
          chan = chan1[0]+"+"+chan2[0]
          filePrefix = "-".join([tag,ifo,chan,str(start),str(end)])
          ## Add omegaveto nodes
          # chan1 and chan2 might have different frame type
          dagNodeName = "omegaveto-%s-%d-%d"%(chan,start,end)
          configFile = os.path.join("configs","%s_%s.conf"%(tag,chan))
          outDir = os.path.join(cp.get("output","outDir"), "-".join([tag,ifo,chan]), "%d_%d"%(start,end))
          ov = omegavetoNode(omegaveto_job,dagNodeName,retry,segfile_name,configFile,dfH.get_output(),dfXs[chan1[1]].get_output(),chan2Frame,outDir,filePrefix,chanData['GW'][2],chan1[2])
          added_parents = []
          ov.add_parent(dfH)
          added_parents.append(dfH)
          if dfXs[chan1[1]] not in added_parents:
            ov.add_parent(dfXs[chan1[1]])
            added_parents.append(dfXs[chan1[1]])
          if chan2Frame is not None and dfXs[chan2[1]] not in added_parents:
            ov.add_parent(dfXs[chan2[1]])
          if not cp.getboolean("condor","only_postprocess"):
            dag.add_node(ov)
          if i == 0:
            report_parents[chan] = [ov]
          else:
            report_parents[chan].append(ov)

    # add reportPage node
    for c in report_parents.keys():
      filePrefix = "-".join([tag,ifo,c])
      dagNodeName = "report-%s"%c
      outDir = os.path.join(cp.get("output","outDir"),"-".join([original_tag,ifo,c]))
      if c.split("+")[-1] == "LINEAR":
        couplingModel = "linear"
      else:
        couplingModel = "bilinear"
      SNRcutoffH = chanData['GW'][2]
      SNRcutoffX = [ch[2] for ch in chanData['fast'] if ch[0] == c.split("+")[0]][0]
      rn = reportNode(report_job,dagNodeName,retry,outDir,filePrefix,couplingModel,SNRcutoffH,SNRcutoffX)
      for n in report_parents[c]:
        if not cp.getboolean("condor","only_postprocess"):
          rn.add_parent(n)
      dag.add_node(rn)
      summary_parents.append(rn)
    
    # add summaryPage node
    filePrefix = "-".join([tag,ifo,str(start),str(end)])
    dagNodeName = "summaryPage-%s-%d-%d"%(tag,start,end)
    sp = summaryPageNode(summaryPage_job,dagNodeName,retry,filePrefix)
    for p in summary_parents:
      sp.add_parent(p)
    dag.add_node(sp)

    # add insert node
    if cp.getboolean("veto","xml") or cp.getboolean("veto","insert"):
      dagNodeName = "insert-%s-%d-%d"%(tag,start,end)
      ins = insertNode(insert_job,dagNodeName,retry,filePrefix)
      ins.add_parent(sp)
      dag.add_node(ins)       
   
    # output workflow as DAG or script; assumes that parents always appear
    # before children.
    dag.add_maxjobs_category("omegaveto",cp.getint("condor","maxOmegavetoJobNum"))
    dag.add_maxjobs_category("report",cp.getint("condor","maxReportJobNum"))
    dag.write_sub_files()
    dag.write_dag()
    if opts.write_script:
        dag.write_script()
    
# =============================================================================
#
#                                  MAIN
#
# =============================================================================

    
# parse commandline
opts = parse_commandline()

if opts.verbose:
	print "Parsed command line arguments..."

# access configuration file
cp = ConfigParser.ConfigParser()
cp.read(opts.ini_file)

if opts.verbose:
	print "Read ini files..."

home = os.environ['HOME']
user = os.environ['USER']

if os.path.isdir("dags"): 
    bcvUtils.rename("dags")
os.makedirs("dags")

global trigger_type
trigger_type = cp.get("veto","trigger_type")

if opts.verbose:
  print "checking the parameters..."


############################################################################
# check necessary txt files
# if not exist, download
############################################################################
inputfiles = {}
if not os.path.isdir("inputfiles"): os.mkdir("inputfiles")

for f in ["vstyle.css"]:
  if not os.path.isfile("inputfiles/%s"%f):
    inputfile = "https://virgo.physics.carleton.edu/public/omegaveto_inputfiles/%s"%f
    outfile = os.path.join("inputfiles",f)
    exit = os.system("wget -O %s %s --no-check-certificate"%(outfile,inputfile))
    if exit > 0:
      print >> sys.stderr, "Error: could not download %s. Please check the address."%cp.get("input","veto_definer_file")
      sys.exit(1)


############################################################################
# check config and make sure they are sane
############################################################################

########################### [general] section ##############################

if cp.get("general","tag")=="":
    from time import strftime, localtime
    cp.set("general","tag",strftime("%m%d%y",localtime()))

if cp.get("general","tag").find("-") is not -1:
    print >> sys.stderr, 'Error: you can not use "-" in your tag.'
    sys.exit(1)

if cp.get("general","ifo") not in ("H1","L1","V1"):
    print >> sys.stderr, "Error: ifo in [general] must be on of H1, L1 or V1."
    sys.exit(1)

ifo = cp.get("general","ifo")
    
########################### [condor] section ###############################

# log directory - set it to /usr1/${USER} if left blank in param file
#                 otherwise set as specified
if cp.get("condor", "logdir")=="":
    cp.set("condor","logdir","/usr1/%s"%user)
else:
    # print precaution message
    print >> sys.stderr, """
********************************************************************************
You might need to tell Condor not to complain that your DAG logs are on 
NFS volumes before submitting your DAG.

bash users:
export _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR=FALSE

tcsh users:
setenv _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR FALSE
********************************************************************************
    """
if cp.get("condor", "scratchdir")=="":
    cp.set("condor","scratchdir","/usr1/%s"%user)

for bin in ("datafind","getKW","omegaveto_wrapper","compiled_omegaveto","veto_report","veto_summaryPage"):
  if not os.path.isfile(cp.get("condor","%s"%bin)):
    print >> sys.stderr, "Error: %s not found"%bin
    sys.exit(1)
    
    
if cp.get("condor","retry")=="":
    print >> sys.stderr, """
    retry is not set in param file.
    setting retry to 1
    """
    cp.set("condor","retry","1")

if cp.get("condor","maxOmegavetoJobNum")=="":
    print >> sys.stderr, """
    maxOmegavetoJobNum is not set in param file.
    setting maxOmegavetoJobNum to 300
    """
    cp.set("condor","maxOmegavetoJobNum","300")

if cp.get("condor","maxReportJobNum")=="":
    print >> sys.stderr, """
    maxReportJobNum is not set in param file.
    setting maxReportJobNum to 10
    """
    cp.set("condor","maxReportJobNum","10")

if cp.get("condor","duration_upper_limit")=="":
    print >> sys.stderr, """
    duration_upper_limit is not set in param file.
    setting retry to 24
    """
    cp.set("condor","duration_upper_limit","24")

if cp.get("condor","duration_lower_limit")=="":
    print >> sys.stderr, """
    duration_lower_limit is not set in param file.
    setting retry to 8
    """
    cp.set("condor","duration_lower_limit","8")

if cp.get("condor","omegavetoRequiredMemory")=="":
    print >> sys.stderr, """
    omegavetoRequiredMemory is not set in param file.
    setting retry to 800
    """
    cp.set("condor","omegavetoRequiredMemory","800")

if cp.get("condor","reportRequiredMemory")=="":
    print >> sys.stderr, """
    reportRequiredMemory is not set in param file.
    setting retry to 800
    """
    cp.set("condor","reportRequiredMemory","800")

for o in ("retry","maxOmegavetoJobNum","maxReportJobNum","duration_upper_limit","duration_lower_limit","omegavetoRequiredMemory","reportRequiredMemory"):   
  try:
    cp.getint("condor",o)
  except ValueError:
    raise('Error: %s must be an int value'%o)

if cp.getint("condor","duration_upper_limit") == 0:
  print >> sys.stderr, "Error: duration_upper_limit must be positive."
  sys.exit(1)
  
if cp.getint("condor","duration_upper_limit") < cp.getint("condor","duration_lower_limit"):
  print >> sys.stderr, "Error: duration_upper_limit must be larger than duration_lower_limit."
  sys.exit(1)

if cp.get("condor","only_postprocess") == "":
  cp.set("condor","only_postprocess","False")

try:
  cp.getboolean("condor","only_postprocess")
except ValueError:
  raise('Error: only_postprocess must be a boolean')

######################## [datafind] section ################################

if cp.get("datafind","url-type") == "":
  cp.set("datafind","url-type","file")

if cp.get("datafind","url-type") not in ("file","gsiftp"):
  print >> sys.stderr, "Error: url-type must be file or gsiftp"
  sys.exit(1)

######################## [data_conditioning] section #######################

# set default values if empty
if cp.getint("data_conditioning","SNRcutoff") == "":
  cp.set("data_conditioning","SNRcutoff","8")

if cp.getfloat("data_conditioning","highPassCutoff") == "":
  cp.set("data_conditioning","highPassCutoff","32")

if cp.getint("data_conditioning","sampleFrequency") == "":
  cp.set("data_conditioning","sampleFrequency","4096")

if cp.getfloat("data_conditioning","reqAccVetoRate") == "":
  cp.set("data_conditioning","reqAccVetoRate","1.66e-06")

if cp.get("data_conditioning","rMax") == "":
  cp.set("data_conditioning","rMax","False")

if cp.getfloat("data_conditioning","vetoSignificanceThresh") == "":
  cp.set("data_conditioning","vetoSignificanceThresh","10")

if cp.getfloat("data_conditioning","positive_window") == "":
  cp.set("data_conditioning","positive_window","0.5")

if cp.getfloat("data_conditioning","negative_window") == "":
  cp.set("data_conditioning","negative_window","0.5")

if cp.getfloat("data_conditioning","safety_thresh") == "":
  cp.set("data_conditioning","safety_thresh","-3")

if cp.getint("data_conditioning","timeShiftMin") == "":
  cp.set("data_conditioning","timeShiftMin","-50")

if cp.getint("data_conditioning","timeShiftMax") == "":
  cp.set("data_conditioning","timeShiftMax","50")

if cp.getint("data_conditioning","numTimeShifts") == "":
  cp.set("data_conditioning","numTimeShifts","11")

for d in ("SNRcutoff","highPassCutoff","sampleFrequency","reqAccVetoRate","vetoSignificanceThresh","positive_window","negative_window","safety_thresh","timeShiftMin","timeShiftMax","numTimeShifts"):
  if cp.get("data_conditioning",d)=="":
    print >> sys.stderr, "Error: %s is required in param file."%d
    sys.exit(1)
  if d in ("highPassCutoff","reqAccVetoRate","vetoSignificanceThresh","positive_window","negative_window","safety_thresh"):
    try:
      cp.getfloat("data_conditioning",d)
    except:
      raise("Error: %s must be a float value"%d)
  else:
    try:
      cp.getint("data_conditioning",d)
    except:
      raise("Error: %s must be an int value"%d)

try:
  cp.getboolean("data_conditioning","rMax")
except:
  ranse("Error: rMax must be boolean")


######################### [output] section #################################

# if not specified, set the output of KW_veto_calc to "results"
if cp.get("output","outDir")=="": 
  cp.set("output","outDir","results")

# find out the original tag name if we're rerunning
if cp.getboolean("condor","only_postprocess"):
  original_tag = os.path.commonprefix(os.listdir(cp.get("output","outDir"))).split("-")[0]
else:
  original_tag = cp.get("general","tag")
    
# if output directory already exists, rename to avoid collision;
# create output directory if not exist yet
# leave it if it's postprocess rerun
if not cp.getboolean("condor","only_postprocess"):
  bcvUtils.rename(cp.get("output","outDir"))
  if not os.path.isdir(cp.get("output","outDir")): 
    os.makedirs(cp.get("output","outDir"))
   
if cp.get("output","logFile")=="":
  cp.set("output","logFile","log.txt") 

try:
  cp.getboolean("output","generateReport")
except:
  raise("Error: generateReport must be boolean")

try:
  cp.getint("output","debugLevel")
except:
  raise("Error: debugLevel must be a int value")
        
############################# [input] section #################################

# check if necessary input exists
for f in ("configuration_file","analyzed_seg_files"):
  if cp.get("input",f)=="":
    print >> sys.stderr, "Error: %s is required in param file"%f
    sys.exit(1)
    if not os.path.isfile(cp.get("input",f)):
      print >> sys.stderr, "Error: --%s %s not found"%(f,cp.get(input,f))
      sys.exit(1)


# get categories to apply
cats = {}
for i in range(5):
  i += 1 # cat start from 1
  try:
    cats[i]=cp.getboolean("input","cat%d"%i)    
  except:
    print >> sys.stderr, "cat%s has to be True or False"%i  
    raise

# if at least one of them is True, check server and veto_defiver_file
if any(cats.values()) == True:
  if cp.get("input","veto_definer_file") == "":
    print >> sys.stderr, "Error: you need to specify veto_definer_file."
  # if http, copy it over
  if cp.get("input","veto_definer_file").startswith("http"):
    outfile = os.path.join("inputfiles",os.path.basename(cp.get("input","veto_definer_file")))
    cmd = "wget -O %s %s"%(outfile,cp.get("input","veto_definer_file"))
    exit = os.system(cmd)
    if exit > 0:
      print >> sys.stderr, "Error: could not download %s. Please check the address."%cp.get("input","veto_definer_file")
      sys.exit(1)
    cp.set("input","veto_definer_file",outfile)
    if opts.verbose:
      print "veto cat files retrieved."
  else:
    if not os.path.isfile(cp.get("input","veto_definer_file")):
      print >> sys.stderr, "Error: veto_definer_file %s not found"%cp.get("input","veto_definer_file")

  # check the server
  if cp.get("input","server") == "":
    cp.set("input","server","https://segdb.ligo.caltech.edu")
  if cp.get("input","server").startswith("https://"):
    cmd_ping = "ligolw_segment_query_dqsegdb --ping --segment-url %s"%cp.get("input","server")
    if os.system(cmd_ping) > 0:
      print >> sys.stderr, "Error: problem with segment server. Please check %s works."%cmd_ping
      sys.exit(1)


########################## [webpage] section ###########################

# if not specified, set output directory of webpages to
# ${HOME}/public_html/veto/(tag)_webpage
if cp.get("webpage", "outdir")=="":
  cp.set("webpage","outdir","%s/public_html/veto/"%home)
web_outdir = os.path.join(cp.get("webpage","outdir"),"%s_webpage"%cp.get("general","tag"))

# make output directory
if not os.path.isdir(cp.get("webpage","outdir")):
  os.makedirs(cp.get("webpage","outdir"))

 
############################### [veto] section #############################

# if blank, then set them to False
if cp.get("veto","xml") == "":
  cp.set("veto","xml",False)
if cp.get("veto","insert") == "":
  cp.set("veto","insert",False)
unsafe_list = []

if cp.getboolean("veto","xml") or cp.getboolean("veto","insert"):
  if not os.path.isfile(cp.get("condor","veto_insert-bin")):
    print >> sys.stderr, "Error: file specified in veto_insert-bin not found."
    sys.exit(1)
  if cp.get("veto","server") == "":
    cp.set("veto","server","https://segdb.ligo.caltech.edu")
  if cp.get("veto","web_address") == "":
    print >> sys.stderr, "Error: web_address in veto section is required."
    sys.exit(1)

  unsafe_list = [f.strip().upper() for f in cp.get("veto","unsafe_channels").split(",")]

  print unsafe_list
  if unsafe_list == [""]:
    unsafe_list = []
  
  if cp.get("veto","trigger_type") not in ("KW", "IHOPE", "MBTA", "OMICRON"):
    print >> sys.stderr, "Error: trigger_type must be KW, IHOPE or MBTA or OMICRON"
    sys.exit(1)

'''
############################ [follow_up] section ###########################

if cp.get("follow_up", "follow_up") == "":
  cp.set("follow_up", "follow_up", False)
if cp.getboolean("follow_up","follow_up"):
  if cp.get("condor","KW_veto_followup-bin")=="":
    cp.set("condor","KW_veto_followup-bin",\
               "/archive/home/isogait/pylal/bin/KW_veto_followup")
  if not os.path.isfile(cp.get("condor","KW_veto_followup-bin")):
    print >> sys.stderr, "Error: followup bin not found"
    sys.exit(1)
  if cp.get("follow_up","twindow") == "":
    cp.set("follow_up","twindow",1)
  try:
    cp.getfloat("follow_up","twindow")
  except:
    raise("Error: twindow must be a number")

  if cp.get("follow_up","dq_flag") == "":
    cp.set("follow_up", "dq_flog", True)

  if cp.getboolean("follow_up","dq_flag"):
    if cp.get("follow_up","server") == "":
      cp.set("follow_up","server","https://segdb.ligo.caltech.edu")
    cmd_ping = "ligolw_dq_query --ping --segment-url %s"%cp.get("follow_up","server")
    if cp.get("follow_up","server").startswith("https://") and os.system(cmd_ping) > 0:
      print >> sys.stderr, "Error: problem with segment server. Please check %s works."%cmd_ping
      sys.exit(1)

  if cp.get("follow_up", "wscans") == "":
    cp.set("follow_up", "wscans", True)
  if cp.getboolean("follow_up","wscans"):  
    if cp.get("condor","KW_veto_runOmega-bin")=="":
        cp.set("condor","KW_veto_runOmega-bin",\
               "/archive/home/isogait/pylal/bin/KW_veto_runOmeaga")
    if not os.path.isfile(cp.get("condor","KW_veto_runOmega-bin")):
      print >> sys.stderr, "Error: runOmega bin not found"
      sys.exit(1)

    if cp.get("condor","wscan-bin") == "":
      print >> sys.stderr, "wscan-bin must be set in param file."
      sys.exit(1)    

    for o in ("wscan_config","wscan_framecache"):
      if cp.get("follow_up",o) == "":
        print >> sys.stderr, "Error: %s is required in param file."%o
        sys.exit(1)
      if not os.path.exists(cp.get("follow_up",o)):
        print >> sys.stderr, "Error: %s %s is not found."%(o,cp.get("follow_up",o))
        sys.exit(1)

    try:
      cp.getint("follow_up","wscan_num")
    except:
      raise("Error: wscan_num must be a number")
'''

########################## show parameters #################################
if opts.verbose:
    for s in cp.sections():
        print s+":"
        for i in cp.items(s):
            print i
        print
            

############################################################################
# get config files / KW triggers
############################################################################

if opts.verbose: print "creating config files and getting KW triggers..."
start_time, duration, segment_file, analyze_segments, chanData, GWtrigFile = config_maker()


############################################################################
# get hardware injection times
############################################################################

if cp.get("input","injection_file") != "off":
  if cp.get("input","injection_file") == "":
    if opts.verbose: print "getting HW injection times..."
    injFile = "injectionTimes.txt"
    exit = os.system(" awk '{print $1}' /archive/home/detchar/public_html/S6/HWinjections/%s-Successful-burst-parsed.txt /archive/home/detchar/public_html/S6/HWinjections/%s-Successful-insp.txt | sort > %s"%(ifo,ifo,injFile))
    if exit > 0:
      print >> sys.stderr, "Error: Failed to retrieve injection times."
      sys.exit(1)
    else:
      cp.set("input","injection_file",injFile)

  if not os.path.isfile(cp.get("input","injection_file")):
    print >> sys.stderr, "Error: injection_file %s not found"%cp.get("input","injection_file")
    sys.exit(1)

  if cp.get("data_conditioning","safety_thresh") == "":
    print >> sys.stderr, "Error: --safety_thresh is a required in param file."
    sys.exit(1)
  try:
    cp.getint("data_conditioning","safety_thresh")
  except:
    raise("Error: safety_thresh must be int")

############################################################################
# set up DAG
############################################################################

if opts.verbose: print "creating dag files..."
dag_maker()

##################### print informational message ##########################

dag_prefix = os.path.join("dags",cp.get("general","tag"))

###################################
# run redundancy cleanup
print "Removing redundancy tags..."
cmd = "redClean"
os.system(cmd)
print "Done!"

# copy MATLAB binary source file
print "Copying updated MATLAB binary source file..."
cmd = "copySRC"
os.system(cmd)
print "Done!"

# add accounting group to .sub files
print "Adding accounting group to .sub files..."
cmd = "groupTag"
os.system(cmd)
print "Done!"
##################################

print """
********************************************************************************
Ready to run the dag file!
To run:
$ condor_submit_dag %s.dag
To check the status:
$ tail -f %s.dag.dagman.out
or
$ condor_q ${USER}
********************************************************************************
"""%(dag_prefix,dag_prefix)

